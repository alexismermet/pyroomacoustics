\documentclass[11pt,a4paper,titlepage]{report}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{float}
\usepackage{url}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage[font=small]{caption} 	
\providecommand{\tightlist}{%
	\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[backend=biber,
sorting=none]{biblatex}
\addbibresource{biblio.bib}
% todo notes
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\author{MERMET Alexis}
\title{Bachelor Project: Augmenting \textit{pyroomacoustics} with machine learning utilities}
\date{June 8, 2018}

\begin{document}
\maketitle
\tableofcontents
\newpage
%...
\chapter{Introduction}
\section{Objectives of the project}
\hspace*{0.6cm}
During this project, we want to implement new functionalities to the already existing Python library, \textit{pyroomacoustics}\cite{pyroom}. These functionalities include a wrapper to Google's Speech Commands Dataset\cite{googlespeech}, utilities for augmenting  datasets with the already-available room impulse response (RIR) generator, and scripts for evaluating the performance of single and multi-microphone processing for speaker recognition against a pre-trained model.\\
\\
\hspace*{0.6cm}
Before I start, I would like to thank Eric Bezzam for the help he gave me during all the semester, by meeting me every week and helping me really quick when I had a problem.
I also would like to thank Robin Scheibler who gave me feedback before my presentations even though he is in Japan and never met me.


\section{What is \textit{pyroomacoustics}?}

\hspace*{0.6cm}
First of all \textit{pyroomacoustics} is a library allowing us to make audio room simulation  and also apply array processing algorithm in Python. Developed by former and current EPFL undergraduate and graduate students, the goal of this library is to aid in ``the rapid development and testing of audio array processing algorithms.''\cite{pyroom} There are three core components:
\begin{enumerate}
	\tightlist
	\item Object-oriented interface in Python for constructing 2D and 3D simulation scenarios;
	\item A fast C implementation of the image source model for room impulse response (RIR) generation;
	\item Reference implementations of popular algorithms for beamforming, direction finding, and adaptive filtering.
\end{enumerate} 
\hspace*{0.6cm} 
Before the start of this project, we could find five main classes in \textit{pyroomacoustics}: The Room class, the SoundSource class, the MicrophoneArray class, the Beamformer class and the STFT class . Quickly after I began working, Robin Schleiber also added a Dataset class that helped me to start creating a wrapper for Google's Speech Commands Dataset (explained below).\\
\\
\hspace{0.6cm}
With the Room class, you create an object that is a collection of Wall objects, a MicrophoneArray and a list of SoundSource(s). It can be either 2D or 3D.
A SoundSource object has as attributes the location of the source itself and also all of its images sources. In general we create this list directly in the Room object that contains the source.
Finally the MicrophoneArray class consist of an array of microphone locations together with a sampling frequency.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{rapport1}
	\caption[Example of a room in \emph{pyroomacoustics}]{Example of a room in \emph{pyroomacoustics}.}
\end{figure}

\section{What is TensorFlow?}
\hspace*{0.6cm}
As they say on their website, TensorFlow is an``open source software library for high performance numerical computation."\cite{TensorFlow} We followed a TensorFlow tutorial called ``Simple Audio Recognition" to create the neural networks we used during the whole project (we are going to explain how it was created in~\ref{sec:training}). We have also reimplemented some of their functions to be able to label sounds we have modified through processing (see~\ref{sec:label_file}).
\chapter{Theoretical knowledge}
\section{Training the Neural Network}
\label{sec:training}
\hspace*{0.6cm}
In this section we're going to talk about how the neural network was trained. First off all, we download the ``Google's Speech Commands''\cite{TensorFlow} dataset since we need it to train our network but to test the performance of our algorithms. According to the tutorial, this model is considered really simple but is also ``quick to train and easy to understand".\\
\hspace*{0.6cm}
This model works as a convolutional network (in fact this model is similar to the one you can use to do some image recognition). First of all a window of time is defined and the audio signal is converted to an ``image'', i.e.\ a 2D array, with the Short Time Fourier Transform (STFT). This is done by ``grouping the incoming audio samples into short segments and calculating the strength of the frequencies across a set of bands". All the frequency strengths of a given segment will then be treated as a vector of values. These vectors are then ordered according to the time, forming the two-dimensional array known as a ``spectrogram''\\
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{Rapport2}
	\caption{Spectrogram of one of our sample during the training (image of the TensorFlow website).}
	\label{fig:spec}
\end{figure}

\hspace*{0.6cm}
In the Figure~\ref{fig:spec}, time is increasing from top to bottom and frequencies from left to right. We can also see different part of the sound that are probably specific part of it (like a syllable in our case of word).\\
\hspace*{0.6cm}
After our ``image'' is created we can feed it into a multi-layer convolutional neural network (CNN), with a fully-connected layer followed by a softmax at the end. With a large amount of images and associated labels, we can train our network to classify different words. It took between 16h-20h to train the model and we're going to look at its accuracy later on in this report.


\section{The GoogleSpeechCommands Dataset: Basic informations}
\label{sec:google}
\hspace*{0.6cm}
Created by the TensorFlow and AIY teams, the Speech Commands dataset is used to add training and inference in TensorFlow. The dataset contains 65,000 one-second long sound of 30 short words, spoken by ``thousands of different people".\cite{googlespeech} This dataset is not fixed and will continue to increase with the contribution of users. It is designed to help a user to create his own basic voice recognition interface, with common words like `yes', `no', directions, etc. The network explained above is trained to recognize the following words:
\begin{enumerate}
	\tightlist
	\item Yes
	\item No
	\item Up
	\item Down
	\item Left
	\item Right
	\item On
	\item Off
	\item Stop
	\item Go
\end{enumerate}


\section{Signal-to-noise ratio}
\label{sec:Signal processing concepts}
\hspace*{0.6cm}
In this section, we're going to talk about one of the most important metrics to quantify the performance of signal processing: the signal-to-noise ratio (SNR)\cite{SNR}. Even though this idea is quite simple and well-known, we talk about it because it is important in our data augmentation and analysis. We are going to talk about it in the ``Implementation" part (Chapter 4).\\
\hspace*{0.6cm}
First of all, for a single sensor, considering the signal $ y(t) = s(t) + n(t) $, with $ s(t) $ being the sound of interest and $ n(t) $ being the noise, the SNR is defined as the signal power over the noise power (assuming zero-mean):
\begin{equation}
SNR_{sensor} = \dfrac{E[|s(t)^2|]}{E[|n(t)^2|]} = \dfrac{\sigma_{s}^2}{\sigma_{n}^2} 
\end{equation}
with $\sigma_{s}^2$ being the power of the ignal of interest and $\sigma_{n}^2$ being the power of the noise.\\
Finally in this project we don't use the SNR under this form but express it in decibels (dB) such that we now have:
\begin{equation}
SNR_{dB} = 10\log_{10}{\dfrac{\sigma_{s}^2}{\sigma_{n}^2}} = 20\log_{10}{\dfrac{\sigma_{s}}{\sigma_{n}}} 
\end{equation}
\hspace*{3.4cm}(cause $\log_{10}{t^2} = 2\log_{10}{t}$).

\section{Algorithms}
\hspace*{0.6cm}
In this section we are going to talk about the different algorithms we used in this project.
\subsection{Single Noise Channel Removal (SNCR)}
\label{sec:SNCR}
\hspace*{0.6cm}
SNCR is used to suppress the noise, stationary noise in particular\cite{SNCR}. We consider a noisy input signal $ x[n] $ that becomes $ X(k,i) $, with $ i $ referring to specific time chunk and $ f $ to frequency. This conversion to the frequency domain is done using the STFT (Short Time Fourier Transform) for overlapping chunks of audio. The noise suppressor removes the noise by applying a time-frequency-varying real-valued gain filter $ G(k,i) $ to $ X(k,i) $. We define this gain filter has follow:
\begin{itemize}
	\tightlist
	\item If there is no noise at a given time and frequency, the gain filter has value 1.
	\item If there is only noise at a given time and frequency, the gain filter has value $ G_{min} $. We choose this value as the one at which the noise shall be attenuated. For example if we want to reduce by 10dB then \begin{equation} G_{min} = 10^{-10/20}
	\end{equation}
	\item If there is a mix of signal and noise at a given time and frequency, the gain filter has a value within $ [G_{min}, 1] $.
\end{itemize}
For this algorithm, an estimation of the noise is needed, we have:
 \[P(k,i) = E[|X(k,i)|^2]  \]
as the estimate of the instantaneous signal + noise power, and we need to compute a noise estimate, $ P_{N}(k,i) $. There is two ways to compute it:
\begin{enumerate}
	\tightlist
	\item If the noise is stationary: $ P_{N}(k,i) = \min P(k,i) $ over some past period of time.
	\item Use a voice detector to determine the most recent chuck corresponding to silence + noise: $ P_{N}(k,i) = P(k,i) $ during this silence period.
\end{enumerate}
In our implementation, we chose the first option, looking back for a fixed number of blocks $ B $:
\begin{equation}
P_{N}(k,i) = \min_{[i- B, \hspace{0.05cm}i]} P(k,i)
\end{equation} 

Now we can define our gain filter such that:
\begin{equation}
G(k,i) = \max[\frac{(P(k,i)- \beta P_{N}(k,i))^\alpha}{P(k,i)^\alpha}, G_{min}]
\end{equation}
where $\beta$ is an overestimation factor, often set to a value larger than 1 to ensure all noise is suppressed by a factor of $G_{min}$, and the exponent $\alpha$ controls the transition behaviour of the gain filter between $G_{min}$ and 1.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{rapport4}
	\caption{STFT-based stationary noise suppressor (from the Audio Signal Processing and Virtual Acoustics books\cite{SCNR}).}
	\label{fig:rapport4}
\end{figure}\\
\hspace*{0.6cm}
In Figure 2.2 you can see a representation of the noise suppressor we have implemented. First the signal x(n) is converted into X(k,i) in the frequency domain using the STFT. Then for each value X(k,i), the circuit compute its power. It is then used as we said before to estimate the noise (equation 2.6) and create the filters G(k,i) (equation 2.7). Finally we use these filters in the frequency domain by multiplying each X(k,i) by G(k,i)to obtain Y(k,i) that will be then transformed back into the time domain (by using inverse STFT called here ISTFT).

\subsection{Beamforming: Delay and Sum}
\hspace*{0.6cm}
In this section, we are going to talk about one of the most basic \emph{beamforming} algorithm\cite{BEAM}. Beamforming tries to perform an intelligent combination of the sensor signals in order to increase the SNR.\\
If we consider an array of $ M $ sensors and define that the signal received at the sensor $ m $ is $ y_{m}(t) = s_{m}(t) + n_{m}(t) $. We can write $ z(t) $ as a weighted sum of $ \{y_m{t}\}_{m=0}^{M-1} $:
\begin{equation}
z(t) = \sum_{m=0}^{M-1}{w_{m} \cdot y_{m}(t-\Delta_{m})} = z_{s}(t) + z_{n}(t) 
\end{equation}
with $ w_{m} $ being the weight corresponding to the signal $ y_{m} $ and $\Delta_{m}$ being a delay to time-align separate microphones so that the signal arrives at the same moment to each sensor.\\
Now we can write the SNR of our \emph{beamformed} signal $z(t) $ as:
\begin{equation}
SNR_{array} = \dfrac{E[|Z_{s}(t)^2|]}{E[|Z_{n}(t)^2|]} = \dfrac{|\sum_{m}w_{m}|^2 \sigma_{s}^2} {\sum_{m}|w_{m}|^2 \sigma_{n}^2} 
\end{equation}
where we have assumed that the noise is uncorrelated across channels.\\
Delay and Sum Weights also called Delay And Sum (DAS) looks like what we discussed above. This algorithm takes each individual microphone signal and put all of them in phase by doing a delay correction. Then it sums up the delayed signals and normalized by the number of microphone channels.
If we consider an array of $ M $ sensors and define that the signal received at the sensor $ m $ is $x_m(t)$ then we define $ y(t) $ such that:
\begin{equation}
y(t) = \frac{1}{M}\sum_{m=0}^{M-1}{w_{m}\cdot x_{m}(t-\Delta_{m})}
\end{equation}
with $ w_{m} $ corresponding to the weights, used to improve the quality of the recording for the $ m^{th} $ signal $ x_{m}  $ and $\Delta_{m}$ corresponding to the delay chosen to maximize the array's sensitivity to waves propagating from a particular direction.

\chapter{Implementation}
\section{The GoogleSpeechCommands Dataset}
\hspace*{0.6cm}
The ``GoogleSpeechCommands" dataset wrapper was created as a subclass of the ``Dataset'' class that was already implemented in \textit{pyroomacoustics}. This class will load Google's Speech Commands Dataset in a structure that is convenient to be processed. It has four main attributes:
\begin{enumerate}
	\tightlist
	\item the directory where the dataset is located, the \texttt{basedir}.
	\item  A dictionary whose keys are word in the dataset. The values are the number of occurrences for that particular word in a dictionary called \texttt{size\_by\_samples}.
	\item  A list of subdirectories in \texttt{basedir}, where each sound type is the name of a subdirectory, called \texttt{subdirs}.
	\item  And finally \texttt{classes}, the list of all sounds, which is the same as the keys of \texttt{size\_by\_samples}.
\end{enumerate}
There are multiple functions in this class and we're going to review them quickly to give you a general idea of what is possible with this wrapper.
\begin{enumerate}
	\tightlist
	\item  we have the \texttt{init} function that is the builder of our class. When creating a structure containing the Google Speech Command dataset, the user can choose if he wants to download it or not. But he can also choose if he wants to construct just a subset of the whole dataset at the start.\\
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.95\linewidth]{Rapport5}
		\caption{The \texttt{init} function of a GoogleSpeechCommands structure}
		\label{fig:rapport5}
	\end{figure}
	\item The \texttt{build corpus} function that allows the user to build the corpus with some filters, as for example the list of the desired words to be taken from the corpus.\\
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.7\linewidth]{Rapport6}
		\caption{The \texttt{build corpus} function from the wrapper}
		\label{fig:rapport6}
	\end{figure}
	\item The \texttt{filter} function that allows the user to filter the dataset and select samples that match the criterias provided.\\
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.4\linewidth]{Rapport7}
		\caption{The \texttt{filter} function from the wrapper}
		\label{fig:rapport7}
	\end{figure}
\end{enumerate}
\hspace*{0.6cm}
Now that we talk about the wrapper, we need to present also the ``GoogleSample" class that is inheriting from the class ``AudioSample" created beforehand in \textit{pyroomacoustics}. This class allows the user to create an audio object to print it in a nice way, plot the corresponding spectrogram, and listen to the file using the \emph{sounddevice} library.\cite{sounddevice}

\section{How to label a file?}
\label{sec:label_file}
\hspace*{0.6cm}
In this section, we are going to see how a user can label a file (following the example script available on my \textit{pyroomacoustics} fork called \texttt{how\_to\_label\_\\sa\_file}). This example uses the ``GoogleSpeechCommand'' dataset and also the graph we obtained by training TensorFlow neural network.
First of all we rewrote some function of TensorFlow such that we could access the result of labelling. These functions are:
\begin{enumerate}
	\tightlist
	\item \texttt{load\_graph}, that is loading the graph used to label sounds.
	\item \texttt{load\_labels} loads the labels corresponding to the graph (for example: yes, no, etc...)
	\item \texttt{run\_graph} labels a sound and return the prediction (in percentage)
	\item \texttt{label\_wav}, the main function
\end{enumerate}
\hspace*{0.6cm}
In the script, the user needs to specify his label file and his graph file. In our example, he can choose one of the word from the list we saw in Section ~\ref{sec:google} and then label it using the \texttt{label\_wav} function in the following way:
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{Rapport8}
	\caption{The labelling function in the \textit{pyroomacoustics} example}
	\label{fig:rapport8}
\end{figure}\\

Here \texttt{destination} represents the directory in which the file to label is kept, labels\_file the label file, graph\_file the graph obtained from TensorFlow and finally word is the sound you except to obtain with this wav file.

Later on, this construction will become really important to us cause we will use it to test the efficiency of our signal processing algorithms for speech recognition \ref{sec:test}. 

\section{How to synthesize noisy signals}
\label{sec:synth}
\hspace*{0.6cm}
Now we will learn how to synthesize noisy signals in \textit{pyroomacoustics}. First of all we have implemented, in utils, two function to create noisy signal:
\begin{enumerate}
	\tightlist
	\item \texttt{modify\_input\_wav}
	\item \texttt{modify\_input\_wav\_multiple\_mics}
\end{enumerate}
We will only talk about the second one since the first function is just a special case of the second function. It can be done with the functions taking care of multiple microphones case (but we keep the first function since its way easier to use it in the single microphone case since you obtain only one noisy signal at the end and not an array of noisy signal you need to flatten).
\begin{figure}[h!]
	\centering
	\includegraphics[width=1.0\linewidth]{Rapport9}
	\caption{the modify\_input\_wav\_multiple\_mics functions from \textit{pyroomacoustics}}
	\label{fig:rapport9}
\end{figure}
This function will first of all create two rooms, one for the sound source and one for the noise source, of the same dimension as specified by the argument \texttt{dimension}. We can separate them since the operations we are going to do are linear and the sound is independent from the noise. Moreover, we would like to separate them so that we can weight \emph{just} the simulated propagated noise according to a specific SNR. After that the room simulation is done, we recover the signals obtained in both rooms. We normalize the noise signal obtained before creating noisy signals for all SNR values given to the function: the argument \texttt{snr\_vals}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{Rapport10}
	\caption{How to synthesize the noisy signal for each SNR value}
	\label{fig:rapport10}
\end{figure}\\
In the figure above, we compute the noisy signal corresponding to each SNR value. The new noise is obtained by multiplying the normalized noise by a coefficient corresponding to a specified SNR value we would like \emph{at the microphone(s)}. We obtain this coefficient from the formula for the SNR (see~\ref{sec:Signal processing concepts}).
\begin{align}
SNR_{dB} = 20\log_{10}{\dfrac{\sigma_{s}}{\sigma_{n}}}\\ 
\Leftrightarrow \dfrac{SNR_{dB}}{20} = \log_{10}{\dfrac{\sigma_{s}}{\sigma_{n}}}\\  
\Leftrightarrow 10^{\dfrac{SNR_{dB}}{20}} = \dfrac{\sigma_{s}}{\sigma_{n}}\\ 
\Leftrightarrow \dfrac{\sigma_{s}}{10^{\dfrac{SNR_{dB}}{20}}} = \sigma_{n}\\ 
\end{align}
with $\sigma_{n}$ being the square root of the noise's power and also the coefficient we are looking for and $\sigma_{s}$ is computed from the simulated source signal. With the obtained coefficient, we can compute the simulated signal with a desired SNR at the microphone(s) as:
\begin{equation}
y(t) = normalized(x(t) + \sigma_{n} * normalized(n(t)) )
\end{equation}
where $ x(t) $ the simulated sound signal and $ n(t) $ the simulated noise signal for our specified room.\\
\hspace*{0.6cm}
You can see an example of how to use this function in my \textit{pyroomacoustics} fork. It is called \texttt{filter}{how\_to\_synthesize\_a\_signal}. In this example, for a word from the ``GoogleSpeechCommands'' dataset (``no") and an SNR of 20, we obtained the following new noisy signal:
\begin{figure}[h!]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.91\linewidth]{rapport12}
		\caption{original input signal's spectrogram}
		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.91\linewidth]{rapport13}
		\caption{A noisy signal's spectrogram}
		\label{fig:sub2}
	\end{subfigure}
	\caption{original input signal's spectrogram compared to new noisy signal's spectrogram }
	\label{fig:rapport11}
\end{figure}
\section{Testing the performance of the algorithms}
\label{sec:test}
\hspace*{0.6cm}
We have implemented one algorithm in this project, the Single Noise Channel Removal (SNCR). The Delay and Sum (DAS) was already implemented in the package with lots of other \emph{beamforming} algorithm that incorporate more sophisticated approach but did not use. Nevertheless, the example we provide can be extended to these algorithms by modifying the algorithm used for processing.\\

\subsection{SNCR}
\hspace*{0.6cm}
You can see how we implemented it in Figure 3.8 however we will explain how we did it.
First of all we create an STFT object from the \emph{pyroomacoustics} library that allows us to compute the STFT and to use it in an easy and efficient way. We prepare also an array that will contain all of our processed audio at the end.\\
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{rapport11}
	\caption{SNCR algorithm implemented in \textit{pyroomacoustics}}
	\label{fig:rapport14}
\end{figure}\\
Then we implement the algorithm that we run for each SNR values given by the user in this case (example called \texttt{analyse\_improvement\_of\_single\_noise\_\\channel\_removal} in \textit{pyroomacoustics}). We set a counter n to 0 at the start of the algorithm. It tells us in which chunk of the STFT we are at a given moment. After that, we enter in the while loop where we use the STFT object to our advantage. With it we can compute easily the STFT of our input signal (a noisy signal). Then we fill our matrix containing all the previous signal's power estimation and we select its minimum value as our noise's power estimation. Having this estimation we can now compute the value of our filter, called mask in the figure, using the formula (2.6) (see~\ref{sec:SNCR}). Finally we have to update our matrix containing the powers and our counter n before repeating the actions above.\\


\subsection{DAS beamforming}
\hspace*{0.6cm}
Even though we have not implemented the algorithm itself, we have used the DAS algorithm as you can see in Figures 3.9 and 3.10. We have create a function working as the one presented in \texttt{How to synthesize noisy signals} as you will see below (see~\ref{sec:synth}). In this function we directly compute the beamformed signal that then we can use in a script looking like the one presented before (see~\ref{sec:label_file}) to label it.\\

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}[thick,scale=0.6, every node/.style={scale=0.6}]
	\draw  (-5.5,4.5) rectangle (-1.5,2.5) node (v1) {};
	\node at (-3.5,3.5) {Create rooms};
	\draw [-latex] (0.5,4.5) rectangle (4.5,2.5);
	\node at (2.5,3.5) {DAS};
	\draw [-latex] (6.5,4.5) rectangle (10.5,2.5);
	\node at (8.5,3.5) {Create noisy signals };
	\node at (8.5,3) {for each SNR value};
	\draw [-latex] (6.5,1) rectangle (10.5,-1);
	\node at (8.5,0) {Label the noisy signals};
	\node (v2) at (-1.5,3.5) {};
	\node (v3) at (0.5,3.5) {};
	\node (v5) at (6.5,3.5) {};
	\node (v6) at (8.5,2.5) {};
	\node (v7) at (8.5,1) {};
	\draw [-latex] (v2) edge (v3);
	\node (v4) at (4.5,3.5) {};
	\draw [-latex] (v4) edge (v5);
	\draw [-latex] (v6) edge (v7);
	\end{tikzpicture}
	\caption{schema of how our beamforming script works}
	\label{fig:rapport16}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{rapport14}
	\caption{how to create beamformed signals (utils package)}
	\label{fig:rapport13}
\end{figure}


Indeed we create two rooms, one containing the signal and the other one noise. In these room, we have a circular array of microphone plus one placed at the centre of this circle. We simulate these two rooms before applying DAS on both of their output (since this algorithm is linear we can do so). Then we normalize our beamformed noise to be able to set the SNR according to a desired value. To do so we use the norm of the noise at the center microphone. Then we create our SNR factor using the norm of the center microphone in the signal room. Finally we compute our noisy signal as before (see~\ref{sec:synth}).


\chapter{Results}
\hspace*{0.6cm}
In this section we will look at the performance of the two algorithms when we try to label processed sound. First we will talk about SNCR and then about DAS.
\section{Analyse improvement of Single Noise Channel Removal}
\hspace*{0.6cm}
We first tried the SNCR algorithm in a script called:\\ \texttt{analyze\_improvement\_of\_single\_noise\_channel\_removal\_fulldata} available in my \textit{pyroomacoustics} fork. We ran it for subset of size 25 in the GoogleSpeechCommands dataset and for the words ``no", ``yes", ``stop" and ``up"  that the TensorFlow neural network can recognize. This means we have created 1750 samples per word and 25 samples per SNR for a given word. We obtained the following results for the the word ``no" and ``yes":\\
\begin{figure}[h!]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{rapport_no_single_noise_channel_removal}
		\caption{classification of the word no}
		\label{fig:sub3}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{rapport_yes_single_noise_channel_removal}
		\caption{classification of the word yes}
		\label{fig:sub5}
	\end{subfigure}

	\caption{classification comparison between the original noisy signal and a processed version of it using SNCR for samples of size 25}
	\label{fig:rapport12}
\end{figure}\\
We can see in the case of the word ``no" that the algorithm improved the recognition for low SNR (SNR that are the most plausible in real life). For example at a SNR of 10dB we have an improvement of 23$\%$ for the classification which is huge (we go from 25$\%$ of correct recognition by the model to 48$\%$). We can also see that the recognition is always higher when we are using a processed signal.
The result of this test is even more impressive when we are working with the word ``yes" since we have an improvement of the classification of 29$\%$ at a SNR of 10dB (we go from 23$\%$ of correct recognition by the model to 52$\%$).\\
We can conclude that the SNCR is a good algorithm for cleaning the data before giving it to a classifier. The improvement for low SNR are always close or above 20$\%$. But we can't say this is perfect. Even if the algorithm is quick, we can see that we don't achieve more that 50$\%$ of correctness in a majority of the case  for low SNR. This is not that good if we want  to use this algorithm for a vocal recognition system.
\section{Analyse improvement of Beamforming}
\hspace*{0.6cm}
Now we will look at the efficiency of our DAS algorithm. We use the script \texttt{analyze\_improvement\_of\_beamforming\_fulldata} available in my \textit{pyroomacoustics} fork. We ran it for subset of size 25 in the GoogleSpeechCommands dataset and for all the word that the TensorFlow neural network can recognize. This means we have created 1750 samples per word and 25 samples per SNR for a given word. We obtained the following result:\\
\begin{figure}[h!]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{rapport_beamforming_no.png}
		\caption{classification of the word no}
		\label{fig:sub4}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=0.9\linewidth]{rapport_beamforming_yes.png}
		\caption{classification of the word yes}
		\label{fig:sub6}
	\end{subfigure}
	
	\caption{classification comparison between the original noisy signal and a processed version of it using DAS for samples of size 25}
	\label{fig:rapport15}
\end{figure}\\
In the case of the word ``no" we can see that the algorithm improved a lot the recognition for low SNR. At 0dB we have an improvement of 30$\%$ for the classification. This is even better than the improvement of SNCR at higher SNR! In this case we go from 10$\%$ of correct recognition by the model to 48$\%$ (nearly 50$\%$). Also the processed signal is always better recognize by the model. We have even better result when we are working with the word ``yes". We have in this case we have an improvement of 58$\%$ of the classification at a SNR of 10dB (we go from 10$\%$ of correct recognition by the model to 68$\%$).\\
Now we can say that the DAS is an even better algorithm than the SNCR for cleaning the data before giving it to a classifier. The improvement at low SNR seems to be higher than 30$\%$ in general. One more time this algorithm is not perfect (what we want is more than 50$\%$ of good recognition at low SNR). But seeing its performance and knowing that DAS is the simplest form of Beamforming, we could say that beamforming seemed to be one solution to improve efficiently the vocal recognition of a system.
\chapter{Conclusion}
\section{Where are we now?}
\hspace*{0.6cm}
At the end of this project we were able to add a wrapper to Google's Speech Commands Dataset that is actually available in \textit{pyroomacoustics} at the address \url{https://github.com/LCAV/pyroomacoustics/tree/master/pyroomacoustics/datasets} since we have done the pull request for it already(\url{https://github.com/LCAV/pyroomacoustics/pull/30}). We were able to add utilities for augmenting  datasets and to test the efficiency of processing algorithm for improving the recognition's quality of a system. We have seen that SNCR is working quite well for what we are doing right now but we can surely do much better (but we will talk about this at the end). DAS beamforming also improved the recognition, but it would be interested to test the performance of more sophisticated algorithms.
All the scripts we have done are now available on the GitHub of \textit{pyroomacoustics} (my fork in fact) at the address \url{https://github.com/alexismermet/pyroomacoustics/tree/master/examples/final_scripts_for_final}.

\section{What's next?}
In the future and for the improvement of the \textit{pyroomacoustics} library, other students could test and implement other signal processing algorithm (for example just test the other beamforming algorithms already available in the package). They could also create new wrapper for other datasets. Then they could also create a new dataset from scratch for \textit{pyroomacoustics} which could help user to test their algorithms in the best condition possible (also it could be quite fun to record sound). Finally, they could try to find another machine learning model with better accuracy than the model we have used in this project.

\printbibliography[
heading=bibintoc,
title={Bibliography}
]
\end{document}
