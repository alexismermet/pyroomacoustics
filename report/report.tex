\documentclass[11pt,a4paper,titlepage]{report}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[section]{placeins}
\usepackage{float}
\usepackage{url}
\usepackage[font=small]{caption}
\providecommand{\tightlist}{%
	\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% todo notes
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}

\author{MERMET Alexis}
\title{Bachelor Project: Augmenting \textit{pyroomacoustics} with machine learning utilities}
\date{June 8, 2018}

\begin{document}
\maketitle
\tableofcontents
\newpage
%...
\chapter{Introduction}
\section{Objectives of the project}
\hspace*{0.6cm}

\todo{can you look into using bibtex? so that you can cite reference directly in the paper.}

During this project, we want to implement new functionalities to the already existing Python library, \textit{pyroomacoustics}\todo{change mentions of \textit{pyroomacoustics} to this style and check for typos!}. These functionalities include a wrapper to Google's Speech Commands Dataset\todo{add reference here}, utilities for augmenting  datasets with the already-available room impulse response (RIR) generator, and scripts for evaluating the performance of single and multi-microphone processing for speaker recognition against a pre-trained model. 

\section{What is \textit{pyroomacoustics}?}

\hspace*{0.6cm}
First of all Pyroomacoustics is a library allowing us to make audio room simulation  and also apply array processing algorithm in Python. Developed by former and current EPFL undergraduate and graduate students, the goal of this library is to aid in ``the rapid development and testing of audio array processing algorithms.'' There are three core components:
\begin{enumerate}
	\tightlist
	\item Object-oriented interface in Python for constructing 2D and 3D simulation scenarios;
	\item A fast C implementation of the image source model for room impulse response (RIR) generation;
	\item Reference implementations of popular algorithms for beamforming, direction finding, and adaptive filtering.
\end{enumerate} 
\hspace*{0.6cm} 
Before the start of this project, we could find three main classes\todo{there's at least two more for DOA and adaptive filtering} in Pyroomacoustic: The Room class, the SoundSource class and the MicrophoneArray class. Quickly after I began working, Robin Schleiber also added a Dataset class that helped me to start creating a wrapper for Google's Speech Commands Dataset (explained below).\\
\\
\hspace{0.6cm}
With the Room class, you create an object that is a collection of Wall objects, a MicrophoneArray and a list of SoundSource(s). It can be either 2D or 3D.
A SoundSource object has as attributes the location of the source itself and also all of its images sources. In general we create this list directly in the Room object that contains the source.
Finally the MicrophoneArray class consist of an array of microphone locations together with a sampling frequency.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{rapport1}
	\caption[Example of a room in Pyroomacoustic]{Example of a room in Pyroomacoustic.}
\end{figure}

\section{What is Tensorflow?}
\hspace*{0.6cm}
\todo{references!}
As they say on their website, Tensorflow is an "open source software library for high performance numerical computation". We followed a Tensorflow tutorial called "Simple Audio Recognition" to create the neural networks we used during the all project (we are going to explain how it was created in 2.1). We have also reimplemented some of their functions to be able to label sounds we have modified through processing (see~\ref{sec:label_file}).\todo{you can make links between section, images, etc with the ``label'' and ``ref'' tags}
\chapter{Theoretical knowledge}
\section{Training the Neural Network}
\hspace*{0.6cm}
In this section we're going to talk about how the neural network was trained. First off all, we download the GoogleSpeechCommand\todo{this is not the official name so I would change to referring it as ``Google's Speech Commands dataset''. ``GoogleSpeechCommand'' is the name we use for our wrapper} dataset since we need it to train our network but to test the efficiency of our algorithms. According to the tutorial, this model is considered really simple but is also ``quick to train and easy to understand".\\
\hspace*{0.6cm}
This model works as a convolutional network (in fact this model is similar to the one you can use to do some image recognition). First of all a window of time is defined and the audio signal is converted to an image with the Short Time Fourier Transform (STFT), i.e.\ a spectrogram. This is done by \todo{change beginning quotes to ``}"grouping the incoming audio samples into short segments and calculating the strength of the frequencies across a set of bands". All the frequency strengths of a given segment will then be treated as a vector of values. These vectors are then ordered according to the time, forming the two-dimensional array known as a ``spectrogram''\\
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{Rapport2}
	\caption{Spectrogram of one of our sample during the training (image of the Tensorflow website).}
\end{figure}

\hspace*{0.6cm}
In the figure 2.1, time is increasing from top to bottom and frequencies from left to right. We can also different part of the sound that are probably specific part of it (like a syllable in our case of word).\todo{this last sentence is unclear...}\\
\hspace*{0.6cm}
After our ``image'' is created we can feed it into a multi-layer convolutional neural network (CNN), with a fully-connected layer followed by a softmax at the end. With a large amount of images and associated labels, we can train our network to classify different words. It took between 16h-20h to train the model and we're going to look at its accuracy later on in this report.


\section{The GoogleSpeechCommands Dataset: Basic informations}
\hspace*{0.6cm}
Created by the \todo{be consistent with how you mention TensorFlow}tensorflow and AIY teams, the speech command dataset is used to add training and inference in tensorflow. The dataset contains 65,000 one-second long sound of 30 short words, spoke by "thousands of different people". This dataset is not fixed and will continue to increase with the contribution of users. It is designed to help a user to create his own basic voice recognition interface, with common words like `yes', `no', directions, etc...\\


\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{rapport3}
	\caption{The different words our system is able to recognize}
\end{figure}
\todo{Figure 2.2 is bit blurry, perhaps you can list the words separately}

\section{Signal processing concepts}
\hspace*{0.6cm}
In this section we're going to talk about one of the most important concept of signal processing we used during this project: the signal-to-noise ratio (SNR). Even though this concept is quite simple and well known, we will talk about it because we it is important in our data augmentation and analysis when we will talk about the implementation (Chapter 4).\\
\hspace*{0.6cm}
First of all, for a single sensor, considering the signal $ y(t) = s(t) + n(t) $, with $ s(t) $ being the sound of interest and $ n(t) $ being the noise, the SNR is defined as the sound power over the noise power.
\begin{equation}
SNR_{sensor} = \dfrac{E[|s(t)^2|]}{E[|n(t)^2|]} = \dfrac{\sigma_{s}^2}{\sigma_{n}^2} 
\end{equation}
with $\sigma_{s}^2$ being the power of the sound of interest and $\sigma_{n}^2$ being the power of the noise.\\
Then if we consider an array of $ M $ sensors and define that the signal received at the sensor $ m $ is $ y_{m}(t) = s_{m}(t) + n_{m}(t) $. We define the signal for the whole array $ Z(t) $ such that:\todo{not exactly...what you have below is a beamformed signal. the SNR should be relatively the same for each microphone. you should talk about the beamformed SNR in the beamforming section}
\begin{equation}
Z(t) = \sum_{m=0}^{M-1}{w_{m}* y_{m}(t-\Delta_{m})} = Z_{s}(t) + Z_{n}(t) 
\end{equation}
with $ w_{m} $ being the weight corresponding to the signal $ y_{m} $ and $\Delta_{m}$ being a delay that to time-align the separate microphones as the sound does not arrive at the same moment to each sensor.\\
Now we can write the SNR of our beamformed signal $ Z(t) $ as:
\begin{equation}
SNR_{array} = \dfrac{E[|Z_{s}(t)^2|]}{E[|Z_{n}(t)^2|]} = \dfrac{|\sum_{m}w_{m}|^2 \sigma_{s}^2} {\sum_{m}|w_{m}|^2 \sigma_{n}^2} 
\end{equation}
\hspace*{0.6cm}
Finally in this project we don't use the SNR under this form cause it is not really intelligible and easy to use. So we convert it into decibels (dB) such that we now have:\todo{perhaps you can explain why we do 20log10 or 10log10?}
\begin{equation}
SNR_{dB} = 10\log_{10}{\dfrac{\sigma_{s}^2}{\sigma_{n}^2}} 
\end{equation}

\section{Algorithms}
\hspace*{0.6cm}
In this section we are going to talk about the different algorithm we used in this project.
\subsection{Single Noise Channel Removal (SCNR)}
\hspace*{0.6cm}
SCNR is used to suppress the noise, stationary noise in particular. We consider a noisy input signal $ x[n] $ that becomes $ X(k,i) $, once converted into the frequency domain using the STFT (Short Time Fourier Transform)\todo{what are k and i?}. The noise suppressor removes the noise by applying a time-frequency-varying real-valued gain filter $ G(k,i) $ to $ X(k,i) $. We define this gain filter has follow:
\begin{itemize}
	\tightlist
	\item If there is no noise at a given time and frequency, the gain filter has value 1.
	\item If there is only noise at a given time and frequency, the gain filter has value $ G_{min} $.
	\item If there is a mix of signal and noise at a given time and frequency, the gain filter has a value within $ [G_{min}, 1] $.
\end{itemize}
For this algorithm, an estimation of the noise is needed, we have:
 \[P(k,i) = E[|X(k,i)|^2]  \]
as the estimate of the instantaneous signal + noise, and we need to compute a noise estimate, $ P_{N}(k,i) $. There is two ways to compute it:
\begin{enumerate}
	\tightlist
	\item If the noise is stationary: $ P_{N}(k,i) = \min P(k,i) $ over some past period of time.
	\item Use a voice detector: $ P_{N}(k,i) = P(k,i) $ during a silence period.
\end{enumerate}
In our implementation, we chose the first option, looking back for a fixed number of blocks $ B $:
\begin{equation}
P_{N}(k,i) = \min_{[i- B, \hspace{0.05cm}i]} P(k,i)
\end{equation} 

Now we can define our gain filter such that:
\begin{equation}
G(k,i) = \max[\frac{(P(k,i)- \beta P_{N}(k,i))^\alpha}{P(k,i)^\alpha}, G_{min}]
\end{equation}
where $\beta$ is an overestimation factor, often set to a value larger than one to ensure all noise is suppressed by a factor of $G_{min}$, and the exponent $\alpha$ controls the transition behavior of the gain filter between $G_{min}$ and 1.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{rapport4}
	\caption{STFT-based stationary noise suppressor (from the Audio Signal Processing and Virtual Acoustics books).}
	\label{fig:rapport4}
\end{figure}

\subsection{Beamforming: Delay and Sum}
\hspace*{0.6cm}
In this section, we are going to talk about one of the most basic \emph{beamforming} algorithm. Beamforming tries to perform an intelligent combination of the sensor signals in order to increase the SNR. Delay and Sum Weights also called Delay And Sum (DAS) is in fact what we have saw before hand in 2.3.\todo{I would move the discussion of the SNR of multiple microphone here to motivate why you take multiple measurements and time-align them}\\
If we consider an array of $ M $ sensors and define that the signal received at the sensor $ m $ is $x_m(t)$ then we define $ y(t) $ such that\todo{keep lower-case for time and upper-case for frequency}:
\begin{equation}
y(t) = \sum_{m=0}^{M-1}{w_{m}* x_{m}(t-\Delta_{m})}
\end{equation}
with $ w_{m} $ corresponds to the weights for the $ m^{th} $ signal $ x_{m}  $and $\Delta_{m}$ corresponds to the delay chosen to maximize the array's sensitivity to waves propagating from a particular direction.\todo{what is $ w_m $ for DAS?}

\chapter{Implementation}
\section{The GoogleSpeechCommands Dataset}
\hspace*{0.6cm}
The ``GoogleSpeechCommands" dataset wrapper was created as a subclass of the ``Dataset'' class that was already implemented in Pyroomacoustics. This class will load Google's Speech Commands Dataset in a structure that is convenient to be processed. It has four main attributes: the directory where the dataset is located, the "basedir". A dictionary whose keys are word in the dataset. The values are the number of occurrences for that particular word in a dictionary called "size\_by\_samples". A list of subdirectories in "basedir", where each sound type is the name of a subdirectory, called "subdirs". And finally "classes", the list of all sounds, which is the same as the keys of "size\_by\_samples".\\
\\
\hspace*{0.6cm}
There are multiple functions in this class and we're going to review them quickly to give you a general idea of what is possible with this wrapper.\todo{use the ``enumerate'' environment for lists}\todo{checkout the ``listings'' package for code snippets}
\\
\hspace*{0.6cm}
1) we have the "init" function that is the builder of our class. When creating a structure containing the Google Speech Command dataset, the user can choose if he wants to download it or not. But he can also choose if he wants to construct just a subset of the all dataset at the start.\\
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.95\linewidth]{Rapport5}
	\caption{The "init" function of a GoogleSpeechCommands structure}
	\label{fig:rapport5}
\end{figure}
\hspace*{0.6cm}
\\
2) The "build corpus" function that allows the user to build the corpus with some filters, as for example the list of the desired words to be taken from the corpus.\\
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{Rapport6}
	\caption{The "build corpus" function from the wrapper}
	\label{fig:rapport6}
\end{figure}
\\
\hspace*{0.6cm}
3) The "filter" function that allows the user to filter the dataset and select samples that match the criterias provided.\\
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.4\linewidth]{Rapport7}
	\caption{The "filter" function from the wrapper}
	\label{fig:rapport7}
\end{figure}
\\
\hspace*{0.6cm}
Now that we talk about the wrapper, we need to present also the "GoogleSample" class that is inheriting from the class "AudioSample" created beforehand in Pyroomacoustics. This class allows the user to create an audio object to print it in a nice way and also to plot the corresponding spectrogram. 

\section{How to label a file?}
\label{sec:label_file}
\section{How to synthesize noisy signals}
\section{The algorithms}
\chapter{Results}
\section{Analyse improvement of Single Noise Channel Removal}
\section{Analyse improvement of Beamforming}
\chapter{Conclusion}
\section{Where are we now?}
\section{What's next?}
\chapter{Bibliography}
1) arXiv:1710.04196v1 [cs.SD],
	Robin Scheibler, Eric Bezzam,
		Ivan Dokmanic, \textit{"Pyroomacoustics: a python package for audio room simulation and array processing algorithms"},
	Ecole Polytechnique Fédérale de Lausanne (EPFL),
		University of Illinois Urbana-Champaign, USA,
	11 Oct 2017
\\
\\
2) Tensorflow,
	\textit{Simple Audio Recognition},
	13 January 2018,
	\url{https://www.tensorflow.org/versions/master/tutorials/audio_recognition}
\\
\\
3) Pete Warden, Software Engineer, Google Brain Team, 
   Google AI Blog,
   \textit{Launching the Speech Command Dataset}
   24 August 2017
   \url{https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html}
\\
\\
4) Jørgen Grythe, 
	\textit{Array gain and reduction of self-noise}, 
	Norsonic AS, Oslo, Norway,
	2016
\\
\\
5) Christof Faller and Dirk Schröder,
	\textit{Audio Signal Processing and Virtual Acoustics},
	9 September 2015,
\\
\\
6) 10.1109/ICASSP.2015.7178030,
	Robin Scheibler, Ivan Dokmanic, and Martin Vetterli, 
	\textit{Raking Echoes in the time domain }, 
	School of Computer and Communication Sciences
	Ecole Poly technique Federale de Lausanne (EPFL), CH-IOI5 Lausanne, Switzerland,
	2015

\end{document}